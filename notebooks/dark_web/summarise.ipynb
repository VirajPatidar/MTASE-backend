{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "!pip install transformers sentencepiece\n",
    "!pip install beautifulsoup4 nltk scipy\n",
    "!pip install -U scikit-learn\n",
    "!pip install langdetect==1.0.9 googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slow and steady always wins the race. “Failure of one time is not a failure of always, provided, one should take the lesson and correct the mistakes” Slow and steady always wins the race.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Once upon a time, there lived a rabbit and tortoise. The rabbit could run fast. He was very proud of his speed. While the turtle was slow and consistent. \n",
    "\n",
    "One day that tortoise came to meet him. The tortoise was walking very slow as usual. The rabbit looked and laughed at him. \n",
    "\n",
    "The tortoise asked “what happened?”\n",
    "\n",
    "The rabbit replied, “You walk so slowly! How can you survive like this?”. \n",
    "\n",
    "The turtle listened to everything and felt humiliated by the rabbit’s words. \n",
    "\n",
    "The tortoise replied, “Hey friend! You are very proud of your speed. Let’s have a race and see who is faster”. \n",
    "The rabbit was surprised by the challenge of the tortoise. But he accepted the challenge as he thought it would be a cakewalk for him.\n",
    "\n",
    "So, the tortoise and rabbit started the race. The rabbit was as usual very fast and went far away. While the tortoise was left behind. \n",
    "\n",
    "After a while, the rabbit looked behind. \n",
    "\n",
    "He said to himself, “The slow turtle will take ages to come near me. I should rest a bit”. \n",
    "\n",
    "The rabbit was tired from running fast. The sun was high too. He ate some grass and decided to take a nap. \n",
    "\n",
    "He said to himself, “I am confident; I can win even if the tortoise passes me. I should rest a bit”. With that thought, he slept and lost the track of time.\n",
    "\n",
    "Meanwhile, the slow and steady turtle kept on moving. Although he was tired, he didn’t rest. \n",
    "\n",
    "Sometime later, he passed the rabbit when the rabbit was still sleeping. \n",
    "\n",
    "The rabbit suddenly woke up after sleeping for a long time. He saw that the tortoise was about to cross the finishing line. \n",
    "\n",
    "He started running very fast with his full energy. But it was too late. \n",
    "\n",
    "The slow turtle had already touched the finishing line. He has already won the race. \n",
    "\n",
    "The rabbit was very disappointed with himself while the tortoise was very happy to win the race with his slow speed. He could not believe his eyes. He was shocked by the end results.\n",
    "\n",
    "At last, the tortoise asked the rabbit “Now who is faster”. The rabbit had learned his lesson. He could not utter a word. The tortoise said bye to the rabbit and left that place calmly and happily.\n",
    "\n",
    "“Failure of one time is not a failure of always, provided, one should take the lesson and correct the mistakes”\n",
    "\n",
    "Slow and steady always wins the race. Never give up. Always keep going. Even if you are slow, your steadiness and consistency will let you win in any situation. Like the tortoise did. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "summary = model.generate(**tokens)\n",
    "tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/naman/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/naman/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "def low_rank_svd(matrix, singular_count):\n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summariser(DOCUMENT):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
    "    DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
    "    DOCUMENT = DOCUMENT.strip()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(DOCUMENT)\n",
    "\n",
    "    normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "    norm_sentences = normalize_corpus(sentences)\n",
    "\n",
    "    tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "    dt_matrix = tv.fit_transform(norm_sentences)\n",
    "    dt_matrix = dt_matrix.toarray()\n",
    "\n",
    "    vocab = tv.get_feature_names_out()\n",
    "    td_matrix = dt_matrix.T\n",
    "\n",
    "    pd.DataFrame(np.round(td_matrix, 2), index=vocab)\n",
    "\n",
    "    l = len(sentences)\n",
    "\n",
    "    if(l <= 2):\n",
    "        return (\"\\n\".join(np.array(sentences)))\n",
    "\n",
    "    num_sentences = 0\n",
    "\n",
    "    if(l < 10):\n",
    "        num_sentences = 3\n",
    "    elif(l < 100):\n",
    "        num_sentences = int(l/3)\n",
    "    elif(l < 500):\n",
    "        num_sentences = int(l/8)\n",
    "    elif(l < 1000):\n",
    "        num_sentences = int(l/14)\n",
    "    else:\n",
    "        num_sentences = 72\n",
    "\n",
    "    u, s, vt = low_rank_svd(td_matrix, 2)  \n",
    "\n",
    "    term_topic_mat, singular_values, topic_document_mat = u, s, vt\n",
    "\n",
    "    # remove singular values below threshold                                         \n",
    "    sv_threshold = 0.5\n",
    "    min_sigma_value = max(singular_values) * sv_threshold\n",
    "    singular_values[singular_values < min_sigma_value] = 0\n",
    "\n",
    "    salience_scores = np.sqrt(np.dot(np.square(singular_values), \n",
    "                                    np.square(topic_document_mat)))\n",
    "\n",
    "    top_sentence_indices = (-salience_scores).argsort()[:num_sentences]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    # print(\"Summary Length: \" , num_sentences)\n",
    "    return (\"\\n\".join(np.array(sentences)[top_sentence_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there lived a rabbit and tortoise.\n",
      "The rabbit could run fast.\n",
      "He was very proud of his speed.\n",
      "While the turtle was slow and consistent.\n",
      "The tortoise was walking very slow as usual.\n",
      "The rabbit looked and laughed at him.\n",
      "You are very proud of your speed.\n",
      "The rabbit was surprised by the challenge of the tortoise.\n",
      "So, the tortoise and rabbit started the race.\n",
      "After a while, the rabbit looked behind.\n",
      "I should rest a bit”.\n",
      "The rabbit was tired from running fast.\n",
      "I should rest a bit”.\n",
      "The slow turtle had already touched the finishing line.\n",
      "The rabbit was very disappointed with himself while the tortoise was very happy to win the race with his slow speed.\n",
      "At last, the tortoise asked the rabbit “Now who is faster”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/naman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "DOCUMENT = \"\"\"\n",
    "Once upon a time, there lived a rabbit and tortoise. The rabbit could run fast. He was very proud of his speed. While the turtle was slow and consistent. \n",
    "\n",
    "One day that tortoise came to meet him. The tortoise was walking very slow as usual. The rabbit looked and laughed at him. \n",
    "\n",
    "The tortoise asked “what happened?”\n",
    "\n",
    "The rabbit replied, “You walk so slowly! How can you survive like this?”. \n",
    "\n",
    "The turtle listened to everything and felt humiliated by the rabbit’s words. \n",
    "\n",
    "The tortoise replied, “Hey friend! You are very proud of your speed. Let’s have a race and see who is faster”. \n",
    "The rabbit was surprised by the challenge of the tortoise. But he accepted the challenge as he thought it would be a cakewalk for him.\n",
    "\n",
    "So, the tortoise and rabbit started the race. The rabbit was as usual very fast and went far away. While the tortoise was left behind. \n",
    "\n",
    "After a while, the rabbit looked behind. \n",
    "\n",
    "He said to himself, “The slow turtle will take ages to come near me. I should rest a bit”. \n",
    "\n",
    "The rabbit was tired from running fast. The sun was high too. He ate some grass and decided to take a nap. \n",
    "\n",
    "He said to himself, “I am confident; I can win even if the tortoise passes me. I should rest a bit”. With that thought, he slept and lost the track of time.\n",
    "\n",
    "Meanwhile, the slow and steady turtle kept on moving. Although he was tired, he didn’t rest. \n",
    "\n",
    "Sometime later, he passed the rabbit when the rabbit was still sleeping. \n",
    "\n",
    "The rabbit suddenly woke up after sleeping for a long time. He saw that the tortoise was about to cross the finishing line. \n",
    "\n",
    "He started running very fast with his full energy. But it was too late. \n",
    "\n",
    "The slow turtle had already touched the finishing line. He has already won the race. \n",
    "\n",
    "The rabbit was very disappointed with himself while the tortoise was very happy to win the race with his slow speed. He could not believe his eyes. He was shocked by the end results.\n",
    "\n",
    "At last, the tortoise asked the rabbit “Now who is faster”. The rabbit had learned his lesson. He could not utter a word. The tortoise said bye to the rabbit and left that place calmly and happily.\n",
    "\n",
    "“Failure of one time is not a failure of always, provided, one should take the lesson and correct the mistakes”\n",
    "\n",
    "Slow and steady always wins the race. Never give up. Always keep going. Even if you are slow, your steadiness and consistency will let you win in any situation. Like the tortoise did. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary = extractive_summariser(DOCUMENT)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l5satz5qzvht225nogtlv42gsq7hruzgi5rw2whal6xt5iq2t5gncjyd.onion', 'wubugnkrzwfyq5oj.onion', 'fr-be.wordpress.org']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all files and directories\n",
    "path = \"/home/naman/Documents/projects/mtase/dark_web/testing\"\n",
    "dir_list = os.listdir(path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files = {}\n",
    "\n",
    "for i in dir_list:\n",
    "\thtml_files.update({i : []})\n",
    "\tpath = \"/home/naman/Documents/projects/mtase/dark_web/testing/\" + i\n",
    "\tfile_list = os.listdir(path)\n",
    "\t# print(file_list)\n",
    "\tfor j in file_list:\n",
    "\t\tif (j.endswith(\".html\")):\n",
    "\t\t\thtml_files[i].append(j)\n",
    "\n",
    "print(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = html_files.keys()\n",
    "path = \"/home/naman/Documents/projects/mtase/dark_web/testing/\"\n",
    "\n",
    "for folder in folders:\n",
    "\tfiles = html_files[folder]\n",
    "\tos.mkdir(path + f\"summary_{folder}\")\n",
    "\tprint(f'\\nIn folder {folder}')\n",
    "\tfor file in files:\n",
    "\t\t# get file\n",
    "\t\t# print(f'path: {path}')\n",
    "\t\t# print(f'file: {file}')\n",
    "\t\t\n",
    "\t\tnew_path = path + folder + \"/\"\n",
    "\t\tsave_path = path + f\"summary_{folder}\" + \"/\"\n",
    "\t\t\n",
    "\t\t# get contents\n",
    "\t\tpage = open(new_path+file, \"r\")\n",
    "\t\tsoup = BeautifulSoup(page, \"html.parser\")\n",
    "\t\thtml_text = soup.get_text()\n",
    "\t\tfor script in soup([\"script\", \"style\"]):\n",
    "\t\t\tscript.extract()\n",
    "\t\t\n",
    "\t\t# parse content\n",
    "\t\ttext = soup.get_text()\n",
    "\t\tlines = (line.strip() for line in text.splitlines())\n",
    "\t\tchunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\t\ttext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "\t\t# translate to english\n",
    "\t\tresult_lang = detect(text)\n",
    "\t\t# print(f\"result lang: {result_lang}\")\n",
    "\t\t# print(f'before text: {text}')\n",
    "\t\tif result_lang != \"en\":\n",
    "\t\t\ttranslator= Translator()\n",
    "\t\t\ttranslation = translator.translate(text, src=result_lang, dest='en')\n",
    "\t\t\ttext = translation.text\n",
    "\t\t\t# print(f'after text: {text}')\n",
    "\t\t# text = detect_and_translate(text, target_lang='en')\n",
    "\n",
    "\t\t# extractive summary\n",
    "\t\textractive = extractive_summariser(text)\n",
    "\n",
    "\t\t# abtractive summary\n",
    "\t\ttokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\t\tsummary = model.generate(**tokens)\n",
    "\t\tabstractive = tokenizer.decode(summary[0])\n",
    "\t\tfin_text = f'Abstractive: {abstractive}\\n\\n\\nExtractive: {extractive}'\n",
    "\n",
    "\t\t# write to file\n",
    "\t\twith open(save_path+file.split()[0]+\".txt\", \"w+\") as f:\n",
    "\t\t\tf.write(fin_text)\n",
    "\t\t\n",
    "\t\tprint(f'Summary for {new_path+file} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
